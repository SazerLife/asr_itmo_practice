{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0eabe",
   "metadata": {},
   "source": [
    "# Языковые модели\n",
    "\n",
    "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты. В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
    "\n",
    "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" итд. Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
    "\n",
    "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель. \n",
    "\n",
    "Практическая работа разделена на 2 части: \n",
    "1. Построение нграмой языковой модели - основная часть, 12 баллов\n",
    "1. Предсказание с помощью языковой модели - дополнительная часть, 4 балла\n",
    "\n",
    "\n",
    "\n",
    "Полезные сслыки:\n",
    "* arpa формат - https://cmusphinx.github.io/wiki/arpaformat/\n",
    "* обучающие материалы - https://pages.ucsd.edu/~rlevy/teaching/2015winter/lign165/lectures/lecture13/lecture13_ngrams_with_SRILM.pdf\n",
    "* обучающие материалы.2 - https://cjlise.github.io/machine-learning/N-Gram-Language-Model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd5c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c1d7",
   "metadata": {},
   "source": [
    "# 1. Построение нграмной языковой модели. (12 баллов)\n",
    "\n",
    "\n",
    "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле: \n",
    "$$ P(w_1, w_2, .., w_n) = {\\prod{{P_{i=0}^{n}(w_i| w_{i-order}, .., w_{i-1})}}} $$\n",
    "\n",
    "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. Формула выглядит следующим образом:\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i})} \\over {count(w_{i-order},..., w_{i-1})}} $$\n",
    "\n",
    "Поскольку униграмы не содержат в себе какого-дибо контекста, вероятность униграмы можно посчитать поделив кол-во этой слова на общее количество слов в обучающей выборке. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5837fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # в первую очередь нам понадобится подсчитать статистику по обучающей выборке \n",
    "# def count_ngrams(train_text: List[str], order=3, bos=True, eos=True) -> Dict[Tuple[str], int]:\n",
    "#     ngrams = defaultdict(int)\n",
    "#     # TODO реализуйте функцию, которая подсчитывает все 1-gram 2-gram ... order-gram ngram'ы в тексте\n",
    "    \n",
    "#     # \n",
    "#     return dict(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edc1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(train_text: List[str], order=3, bos=True, eos=True) -> Dict[Tuple[str], int]:\n",
    "    ngrams = defaultdict(int)\n",
    "\n",
    "    for text in train_text:\n",
    "        words = text.split()\n",
    "        words = bos * ['<s>'] + words + eos * ['</s>']\n",
    "\n",
    "        for suborder in range(1, order + 1):\n",
    "            ngrams_count = len(words) - (suborder - 1)\n",
    "            for j in range(ngrams_count):\n",
    "                ngrams[tuple(words[j:j + suborder])] += 1\n",
    "\n",
    "    return dict(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd69d44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1a passed\n"
     ]
    }
   ],
   "source": [
    "def test_count_ngrams():\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=True, eos=True) == {\n",
    "        ('<s>',): 1, \n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=True) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1, \n",
    "        ('</s>',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=1, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1\n",
    "    }\n",
    "    assert count_ngrams(['привет привет как дела'], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 2, \n",
    "        ('как',): 1, \n",
    "        ('дела',): 1,\n",
    "        ('привет', 'привет'): 1,\n",
    "        ('привет', 'как'): 1,\n",
    "        ('как', 'дела'): 1\n",
    "    }    \n",
    "    assert count_ngrams(['привет ' * 6], order=2, bos=False, eos=False) == {\n",
    "        ('привет',): 6, \n",
    "        ('привет', 'привет'): 5\n",
    "    }\n",
    "    result = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие пройдет в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=5)\n",
    "    assert result[('<s>',)] == 3\n",
    "    assert result[('32',)] == 3\n",
    "    assert result[('<s>', 'в', 'офлайне', 'в', '32')] == 1\n",
    "    assert result[('офлайне', 'в', '32', '12', '</s>')] == 1\n",
    "    print('Test 1a passed')\n",
    "    \n",
    "    \n",
    "test_count_ngrams()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e1865",
   "metadata": {},
   "source": [
    "\n",
    "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю. \n",
    "\n",
    "Чтобы избежать данного недостатка, вводится специальное сглаживание - [сглаживание Лапласа](https://en.wikipedia.org/wiki/Additive_smoothing). Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
    "\n",
    "Формула сглаживания Лапласа выглядит следующим образом:\n",
    "\n",
    "$$ P(w_i| w_{i-order}, .., w_{i-1}) = {{count(w_{i-order}, .., w_{i}) + k} \\over {count(w_{i-order},..., w_{i-1}) + k*V}} $$\n",
    "\n",
    "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания. Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cafb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # функция подсчета вероятности через количество со сглаживанием Лапласа\n",
    "# def calculate_ngram_prob(ngram: Tuple[str], counts: Dict[Tuple[str], int], V=None, k=0) -> float:\n",
    "#     # подсчитывет ngram со сглаживанием Лапласа\n",
    "#     # TODO\n",
    "#     return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a9922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_prob(ngram: Tuple[str], counts: Dict[Tuple[str], int], V=None, k=0) -> float:\n",
    "    if V is None:\n",
    "        V = len([ngram for ngram in counts if len(ngram) == 1])\n",
    "\n",
    "    ngram_count = counts.get(ngram, 0)\n",
    "    sub_ngrams_count = sum([counts[ngram] for ngram in counts if len(ngram) == 1])\n",
    "    if len(ngram) > 1:\n",
    "        sub_ngrams_count = counts[ngram[:-1]] if ngram[:-1] in counts else 0 #  + (V * k)\n",
    "\n",
    "    # print(f\"({ngram_count} + {k}) / ({sub_ngrams_count} + {k} * {V})\")\n",
    "    prob = (ngram_count + k) / (sub_ngrams_count + k * V)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6610ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = count_ngrams(['практическое сентября', 'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты', 'в офлайне в 32 12'], order=4)\n",
    "# {ngram: counts[ngram] for ngram in counts if len(ngram) == 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e66075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60b25d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.b passed\n"
     ]
    }
   ],
   "source": [
    "def test_calculate_ngram_prob():\n",
    "    counts = count_ngrams(['практическое сентября',\n",
    "                           'второе практическое занятие в офлайне 32 сентября в 12 часов 32 минуты',\n",
    "                           'в офлайне в 32 12'], order=4)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts) == 0.5\n",
    "    assert calculate_ngram_prob(('в', ), counts) == 4/25\n",
    "    assert calculate_ngram_prob(('в', ), counts, k=0.5) == (4+0.5)/(25+0.5*12)\n",
    "    assert calculate_ngram_prob(('в', 'офлайне', 'в', '32'), counts) == 1.0\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=1) == 0.1875\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=0) == 0.0\n",
    "    assert calculate_ngram_prob(('в', 'онлайне'), counts, k=1) == 0.0625\n",
    "    assert calculate_ngram_prob(('в', 'офлайне'), counts, k=0.5) == 0.25\n",
    "\n",
    "    print(\"Test 1.b passed\")\n",
    "    \n",
    "\n",
    "test_calculate_ngram_prob()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da494bf0",
   "metadata": {},
   "source": [
    "Основной метрикой язковых моделей является перплексия. \n",
    "\n",
    "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "\n",
    "$$ ppl = {P(w_1, w_2 ,..., w_N)^{- {1} \\over {N}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd1f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Языковая модель \n",
    "class NgramLM:\n",
    "    def __init__(self, order=3, bos=True, eos=True, k=1, predefined_vocab=None):\n",
    "        self.order = order\n",
    "        self.eos = eos\n",
    "        self.bos = bos\n",
    "        self.k = k\n",
    "        self.vocab = predefined_vocab\n",
    "        self.ngrams_count = None\n",
    "        \n",
    "    @property\n",
    "    def V(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def fit(self, train_text: List[str]) -> None:\n",
    "        self.ngrams_count = count_ngrams(train_text, self.order, self.bos, self.eos)\n",
    "        self.vocab = [ngram[0] for ngram in self.ngrams_count if len(ngram) == 1]\n",
    "    \n",
    "    def predict_ngram_log_proba(self, ngram: Tuple[str]) -> float:\n",
    "        proba = calculate_ngram_prob(ngram, self.ngrams_count, V=self.V, k=self.k)\n",
    "        log_proba = np.log(proba)\n",
    "        return log_proba\n",
    "           \n",
    "    def predict_log_proba(self, words: List[str]) -> float:\n",
    "        words = self.bos * ['<s>'] + words[0].split() + self.eos * ['</s>']\n",
    "        logprob = 0\n",
    "        start, end = 1 - self.order, 0\n",
    "        while end != len(words):\n",
    "            ngram = tuple(words[start if start > 0 else 0 : end + 1])\n",
    "            logprob += self.predict_ngram_log_proba(ngram)\n",
    "            start, end = start + 1, end + 1\n",
    "        return logprob\n",
    "        \n",
    "    def ppl(self, text: List[str]) -> float:\n",
    "        logprob = self.predict_log_proba(text)\n",
    "        ngrams_count = sum([len(line.split()) + self.bos + self.eos for line in text])\n",
    "        perplexity = np.exp(-logprob / ngrams_count)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bfeb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
    "                  \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
    "                  \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"]\n",
    "lm = NgramLM(order=2)\n",
    "lm.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c470fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.288884851420658 15.288884851420656\n",
      "14.846584407951667 14.84658440795166\n",
      "7.334561964590593 7.33\n"
     ]
    }
   ],
   "source": [
    "print(lm.ppl(['']), ((3+1)/(41 + 14) * 1/(3+14))**(-1/2))\n",
    "print(lm.ppl(['ЧТО']), ((3+1)/(41 + 14) * 1/(3+14) * 1/(14)) ** (-1/3))\n",
    "print(lm.ppl([\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]), 7.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0bfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lm():\n",
    "    train_data = [\"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\",\n",
    "                  \"если я сойду с ума прямо сейчас по-моему мы сэкономим уйму времени\",\n",
    "                  \"мы сэкономим уйму времени если я сейчас сойду с ума по-моему\"]\n",
    "    global lm\n",
    "    lm = NgramLM(order=2)\n",
    "    lm.fit(train_data)\n",
    "    assert lm.V == 14\n",
    "    assert np.isclose(lm.predict_log_proba(['мы']), lm.predict_log_proba([\"если\"]))\n",
    "    assert lm.predict_log_proba([\"по-моему\"]) > lm.predict_log_proba([\"если\"]) \n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14))**(-1/2)\n",
    "    ppl = lm.ppl([''])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    gt = ((3+1)/(41 + 14) * 1/(3+14) * 1/(14)) ** (-1/3)\n",
    "    ppl = lm.ppl(['ЧТО'])\n",
    "    assert  np.isclose(ppl, gt), f\"{ppl=} {gt=}\"\n",
    "    \n",
    "    test_data = [\"по-моему если я прямо сейчас сойду с ума мы сэкономим уйму времени\"]\n",
    "    ppl = lm.ppl(test_data)\n",
    "    assert round(ppl, 2) == 7.33, f\"{ppl}\"\n",
    "test_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafa0a2",
   "metadata": {},
   "source": [
    "# 2. Предсказания с помощью языковой модели (4 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d2eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_next_word(lm: NgramLM, prefix: List[str], topk=4):\n",
    "#     # TODO реализуйте функцию, которая предсказывает продолжение фразы. \n",
    "#     # верните topk наиболее вероятных продолжений фразы prefix \n",
    "#     return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fbea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(lm: NgramLM, prefix: List[str], topk=4):\n",
    "    probabilities = list()\n",
    "    for word in lm.vocab:\n",
    "        ngram = tuple(prefix + [word])\n",
    "        proba = lm.predict_ngram_log_proba(ngram)\n",
    "        probabilities.append((proba, word))\n",
    "    topk_words = [item[1] for item in sorted(probabilities, reverse=True)[:topk]]\n",
    "    return topk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4846b",
   "metadata": {},
   "source": [
    "Обучите языковую модель на всем тексте из этой лабораторной работы (не забудьте заранее нормализовать текст).\n",
    "\n",
    "Что предскажет модель по префиксам `по-моему`, `сэкономим`, `нграм` и `вероятности`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c64fc445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"\"\"\n",
    "Языковые модели играют важную роль в системах распознавания речи, помогая создавать более грамотные и лексически корректные тексты.\n",
    "В данной работе мы будем изучать нграмные языковые модели, которые позволяют довольно легко оценить вероятность и правдоподобность текста.\n",
    "В нграмной языковой модели, нграм - это последовательность из n слов в тексте. \n",
    "Например, в предложении \"по-моему мы сэкономим уйму времени если я сойду с ума прямо сейчас\", биграмами будут \"по-моему мы\", \"мы сэкономим\", \"сэкономим уйму\" и тд.\n",
    "Языковые модели оценивают вероятность появления последовательности слов, исходя из статистики появления каждого из нграм в обучающей выборке.\n",
    "Порядком (order) нграм языковой модели называют максимальную длину нграм, которую учитывает модель.\n",
    "Практическая работа разделена на 2 части:\n",
    "Построение нграмой языковой модели - основная часть, 12 баллов.\n",
    "Предсказание с помощью языковой модели - дополнительная часть, 4 балла.\n",
    "Полезные сслыки.\n",
    "Вероятность текста с помощью нграмной языковой модели можно вычислить по формуле.\n",
    "В простом виде, при обучении нграмной языковой модели, чтобы рассчитать условную вероятность каждой нграмы, используется формула, основанная на количестве появлений нграмы в обучающей выборке. \n",
    "Формула выглядит следующим образом.\n",
    "Простой подход к вычислению вероятностей через количество нграм имеет существенный недостаток. \n",
    "Если в тексте встретится нграмма, которой не было в обучающей выборке, то вероятность всего текста будет равна нулю. \n",
    "Чтобы избежать данного недостатка, вводится специальное сглаживание - сглаживание Лапласа.\n",
    "Данная техника позволяет учитывать нграмы, не встретившиеся в обучающей выборке, и при этом не делает вероятность текста равной нулю.\n",
    "Формула сглаживания Лапласа выглядит следующим образом.\n",
    "Здесь V - количество слов в словаре, а k - гиперпараметр, который контролирует меру сглаживания.\n",
    "Как правило, значение k выбирается экспериментально, чтобы найти оптимальный баланс между учетом редких нграм и сохранением вероятности для часто встречающихся нграм.\n",
    "Основной метрикой язковых моделей является перплексия.\n",
    "Перплексия  — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку.\n",
    "Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "Обучите языковую модель на всем тексте из этой лабораторной работы (не забудьте заранее нормализовать текст).\n",
    "Что предскажет модель по префиксам `по-моему`, `сэкономим`, `нграм` и `вероятности`? \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861fda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text(text: str):\n",
    "    sentences = text.strip().split(\"\\n\")\n",
    "    processed_sentences = list()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = re.sub(r\"[^\\w\\d\\-\\s]\", \" \", sentence).strip()\n",
    "        sentence = re.sub(r\"\\s+-\\s+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "        processed_sentences.append(sentence)\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "107862fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = process_text(train_text)\n",
    "lm = NgramLM(order=2)\n",
    "lm.fit(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67a137e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "по-моему ['мы', 'сэкономим', 'языковые', 'языковую']\n",
      "сэкономим ['уйму', 'сэкономим', 'нграм', 'языковые']\n",
      "нграм ['и', 'языковой', 'это', 'которую']\n",
      "вероятности ['хорошо', 'для', '</s>', 'языковые']\n"
     ]
    }
   ],
   "source": [
    "print(\"по-моему\", predict_next_word(lm, [\"по-моему\"], topk=4))\n",
    "print(\"сэкономим\", predict_next_word(lm, [\"сэкономим\"], topk=4))\n",
    "print(\"нграм\", predict_next_word(lm, [\"нграм\"], topk=4))\n",
    "print(\"вероятности\", predict_next_word(lm, [\"вероятности\"], topk=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b8f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
